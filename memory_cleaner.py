# when cleaning names
# id=\nStuff\n|{{tooltop|Stuff<br>}}
# Save after |
# delete {{tooltop|
# delete <br> or |
# whichever comes after the Name, check for both

import os
import json
import mwparserfromhell as mw
import re
from bs4 import BeautifulSoup


INPUT_FILE = "json_formatted.json"

def name_parse(name: str):
# For stripping leading id and data sort values
    name_split = []
    if "|" in name and "id=" in name or "data-sort-value=" in name:
        name_split = name.split("|", 1)
        name = ""
        if "id=" in name_split[0] or "data-sort-value=" in name_split[0]:
            name_split[0] = ""

# HTML STRIPPER
    for part in name_split:
        part = part.strip()
        soup = BeautifulSoup(part, "html.parser")
        name += soup.get_text()

# Stripping Templates and extra, unneeded data
    wikicode = mw.parse(name)
    for template in reversed(wikicode.filter_templates(recursive=True)): #undo from the inside out
        try:
            val = "" + str(template.get(1).value).strip()
        except ValueError:
            val = ""
        
        if template.name.strip().lower() == "c":
            val = " | " + val # for formatting / style lol. all the {{c| were bonus info, so separating them with a |
        wikicode.replace(template,val)

    return wikicode.strip_code().strip()

def chapter_parse(chapter: str):
    # do the check for id= and data-sort before a | and remove it
    # replace <br> with ", "
    # templace shananigans, can probably copy a lot from the names

    # can copy a lot (most) code from name_parse
    chapter_split = []
    if "|" in chapter and "id=" in chapter or "data-sort-value=" in chapter:
        chapter_split = chapter.split("|", 1)
        chapter = " "
        if "id=" in chapter_split[0] or "data-sort-value=" in chapter_split[0]:
            chapter_split[0] = " "

    chapter = chapter.replace("<br>", " | ")

    wikicode = mw.parse(chapter)
    for template in reversed(wikicode.filter_templates(recursive=True)): #undo from the inside out
        try:
            val = " " + str(template.get(1).value).strip()
        except ValueError:
            val = " "
        
        if template.name.strip().lower() == "c":
            val = " | " + val # for formatting / style lol. all the {{c| were bonus info, so separating them with a |
        wikicode.replace(template,val)


    return wikicode.strip_code().strip() 

def status_parse(status: str):
    status = re.sub("<(.*?)>", " ", status)
#    status = BeautifulSoup(status, "html.parser").get_text()

    wikicode = mw.parse(status)
    for template in reversed(wikicode.filter_templates(recursive=True)): #undo from the inside out
        try:
            val = " " + str(template.get(1).value).strip()
        except ValueError:
            val = " "
        
        if template.name.strip().lower() == "c":
            val = "|" + val # for formatting / style lol. all the {{c| were bonus info, so separating them with a |
        wikicode.replace(template,val)


    return wikicode.strip()

def desc_parse(desc):
    # Need to split up this section based on <div>'s
    soup = BeautifulSoup(desc, 'html.parser')
    # print(soup.prettify())

    parsed = {}

    # can add things that share names by just adding like " | " + text
    # then do .strip( |) to strip spaces and | from the endif thats at the beginning

    clear_divs = ""

    # bone singer could be an issue

    all_divs = soup.find_all("div", recursive = True)
    # need to save beginning part and parse end with divs

    old_div = ""
    for div in reversed(all_divs):
        # right now have <div> aoiwndoa <div> aidunwad </div> </div>
        if old_div:
            div = str(div).replace(old_div, " ")
        old_div = str(div)
        print(old_div)
        
        # grab inner, strip html, replace with 0
       # print(div)
        print("------------")


    # Titles are stored in between the divs, then info after that div
    # <div stuff>Description<div>information</div</div><div>Runes<div>'''Rank'''</div></div>
    # I want to try and separate that out into different json sections, store it inside of desc, and return it
    # use beautiful soup
    # desc = re.sub("<(.*?)>", " ", desc)
    print("-------------------------------------------------")
    return desc.strip()


#def main():
#    if os.path.exists(INPUT_FILE):
with open(INPUT_FILE, "r", encoding="utf-8") as f:
    data = json.load(f)
        # os.remove(INPUT_FILE)
# os.remove("cleaned_memory_names.txt")

all_pages = []

# set to ademndum mode, change to write once json structures are built (in ademndum bc adding in a loop, can add json all at once)
with open("cleaned_memory_names.json", "w", encoding="utf-8") as f:
#     print("test")
    for page in data:
        for page_name, items in page.items():
            for item in items:
                item["Name"] = name_parse(item["Name"])
                # inconsistant naming convention caused this
                # can fix by making new json objects
                # not doing that rn
                if "Chapter Received" in item:
                    item["Chapter Received"] = chapter_parse(item["Chapter Received"])
                elif "Chapter Appeared" in item:
                    item["Chapter Appeared"] = chapter_parse(item["Chapter Appeared"])    
                else:
                    item["Chapter"] = chapter_parse(item["Chapter"])

                item["Status"] = status_parse(item["Status"])
                item["Description"] = desc_parse(item["Description"])
    json.dump(data, f, ensure_ascii=False, indent = 2)
    #f.write(data)
 #   else:
  #      print(INPUT_FILE, "not found.")



# if __name__ == "__main__":
#     main()

